{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16.3 Reduce Overﬁtting With Dropout on the visible Layer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJz9TaJmDBDjid5lwaihdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelounb/Deep_Learning_with_python_JasonBrownlee/blob/master/16_3_Reduce_Over%EF%AC%81tting_With_Dropout_on_the_visible_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwlCKIeB1Xf5",
        "colab_type": "text"
      },
      "source": [
        "A simple and powerful regularization technique for neural networks and deep learning models is dropout. In this lesson you will discover the dropout regularization technique and how to apply it to your models in Python with Keras. After completing this lesson you will know:\n",
        "1. How the dropout regularization technique works.\n",
        "2. How to use dropout on your input and hidden layers.\n",
        "3. How to use dropout on your hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVrtH-W01ivH",
        "colab_type": "text"
      },
      "source": [
        "#Dropout Regularization For Neural Networks\n",
        "Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. \n",
        "\n",
        "Dropout is a technique where randomly selected neurons are ignored during training. They are dropped-out randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. \n",
        "\n",
        "As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for speciﬁc features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. \n",
        "\n",
        "This reliant on context for a neuron during training is referred to as complex co-adaptations. You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network. \n",
        "\n",
        "The effect is that the network becomes less sensitive to the speciﬁc weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overﬁt the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiWASMYV2vBL",
        "colab_type": "text"
      },
      "source": [
        "# Dropout Regularization in Keras\n",
        "\n",
        "Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras. \n",
        "\n",
        "Dropout is only used during the training of a model and is not used when evaluating the skill of the model. Next we will explore a few different ways of using Dropout in Keras. \n",
        "\n",
        "The examples will use the **Sonar dataset** binary classiﬁcation dataset (learn more in Section 11.1). We will evaluate the developed models using scikit-learn with 10-fold cross validation, in order to better tease out differences in the results. \n",
        "\n",
        "There are 60 input values and a single output value and the input values are standardized before being used in the network. The baseline neural network model has two hidden layers, the ﬁrst with 60 units and the second with 30. Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnd0niwI1OvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0191244b-5f7d-42b7-dd68-9edb6912117f"
      },
      "source": [
        "# Baseline Model on the Sonar Dataset \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense \n",
        "from keras.layers import Dropout \n",
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from keras.constraints import maxnorm \n",
        "from keras.optimizers import SGD \n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xC2AUlJ9uJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset \n",
        "dataframe = pd.read_csv(\"sonar.csv\", header=None) \n",
        "dataset = dataframe.values \n",
        "# split into input (X) and output (Y) variables \n",
        "X = dataset[:,0:60].astype(float) \n",
        "Y = dataset[:,60] "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ePd9Qgw92SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility \n",
        "seed = 7 \n",
        "np.random.seed(seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj1WgxpD-Avy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode class values as integers \n",
        "encoder = LabelEncoder() \n",
        "encoder.fit(Y) \n",
        "encoded_Y = encoder.transform(Y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS0lhcfd-I0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a4b23c77-482a-4a17-d845-423d82aac7a6"
      },
      "source": [
        "# baseline \n",
        "\"\"\"def create_baseline(): \n",
        "  # create model \n",
        "  model = Sequential() \n",
        "  model.add(Dense(60, input_dim=60, kernel_initializer= 'normal' , activation= 'relu' )) \n",
        "  model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' )) \n",
        "  model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' )) \n",
        "  # Compile model \n",
        "  sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False) \n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer='sgd', metrics=[ 'accuracy' ]) \n",
        "  return model\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def create_baseline(): \\n  # create model \\n  model = Sequential() \\n  model.add(Dense(60, input_dim=60, kernel_initializer= 'normal' , activation= 'relu' )) \\n  model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' )) \\n  model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' )) \\n  # Compile model \\n  sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False) \\n  model.compile(loss= 'binary_crossentropy' , optimizer='sgd', metrics=[ 'accuracy' ]) \\n  return model\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT5zu65h81VX",
        "colab_type": "text"
      },
      "source": [
        "# Using Dropout on the Visible Layer\n",
        "\n",
        "Dropout can be applied to input neurons called the visible layer. In the example below we add a new Dropout layer between the input (or visible layer) and the ﬁrst hidden layer. \n",
        "\n",
        "The dropout rate is set to 20%, meaning one in ﬁve inputs will be randomly excluded from each update cycle. Additionally, as recommended in the original paper on dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the W constraint argument on the Dense class when constructing the layers. \n",
        "\n",
        "The learning rate was lifted by one order of magnitude and the momentum was increased to 0.9. These increases in the learning rate were also recommended in the original dropout paper. Continuing on from the baseline example above, the code below exercises the same network with input dropout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjMd7zsS8yFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropout in the input layer with weight constraint \n",
        "def create_model(): \n",
        "  # create model \n",
        "  model = Sequential() \n",
        "  model.add(Dropout(0.2, input_shape=(60,))) \n",
        "  model.add(Dense(60, kernel_initializer= 'normal' , activation= 'relu' , kernel_constraint=maxnorm(3))) \n",
        "  model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' , kernel_constraint=maxnorm(3))) \n",
        "  model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' )) \n",
        "  # Compile model \n",
        "  sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False) \n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ]) \n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6eRHu9W_xXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed) \n",
        "estimators = [] \n",
        "estimators.append(( 'standardize' , StandardScaler())) \n",
        "estimators.append(( 'mlp' , KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0))) \n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmfFe32PBrHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61233ca0-535a-438e-f291-dc4a32502e6a"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) \n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold) \n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 84.55% (7.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gjc5IN_s-YH",
        "colab_type": "text"
      },
      "source": [
        "Running the example with dropout in the visible layer provides a nice lift in classiﬁcation accuracy to 86%\n"
      ]
    }
  ]
}